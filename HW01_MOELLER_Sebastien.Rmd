---
title: "Homework 1"
author: "Sebastien Moeller"
date: "29/01/2018"
output: html_document
---

## Dependencies
```{r}
if (!require("pacman")) install.packages("pacman")

pacman::p_load(Rfast, leaps, CVXR)
```

## The objectives of the Lab
The purpose of this lab is to reproduce tables from the third chapter of the book "Elements of
Statistical Learning" from Hastie, Tibshirani and Friedman, as they are shown bellow.
__Ex.1__ Prepare the data
a) Raw data is available on line, download it from moodle (theData.txt file) or from the web at http://statweb.stanford.edu/~tibs/ElemStatLearn.1stEd/datasets/prostate.data.
```{r}
data <- read.table("prostate.data", sep = "")
```

b) Extract and normalize the explicative variables
```{r}
X <- scale(data[,1:8])
```

c) Is it wise to normalize these data?


d) Extract the target variable
```{r}
Y <- as.matrix(data[,"lpsa"])
```

e) Split the dataset into training and test data
```{r}
Xtrain <- X[data[["train"]], ]
Ytrain <- Y[data[["train"]],]
Xtest <- X[!data[["train"]], ]
Ytest <- Y[!data[["train"]], ]
```

2. Compute the correlations of predictors in the prostate cancer data as presented Table 3.1
```{r}
Xtrainscale <- scale(Xtrain)

C <- cov(as.matrix(Xtrainscale))
```

3. Reproduce the results presented Table 3.2
a) Compute the coefficients of the linear regression model, without using the lm function (but you can use it validate your code)
```{r}
Xtrainone <- cbind(array(1, dim = c(nrow(Xtrain),1)), Xtrain)

b <- solve(t(Xtrainone) %*% Xtrainone, t(Xtrainone) %*% Ytrain)
```

b) Compute the prediction error
```{r}
Ypred <- Xtrainone %*% b

err <- Ytrain - Ypred
```

c) Compute the standard error for each variable
```{r}
sig2 <- (t(err) %*% err)/ (nrow(Xtrainone) - ncol(X) -1)

v <- diag(solve(t(Xtrainone) %*% Xtrainone))

stderr <- sqrt(as.vector(sig2)) * sqrt(v)
```

d) compute the Z score for each variable
```{r}
Z <- b/stderr
```
e) visualize the results and compare with table 3.2
```{r}
table32 <- cbind(b,stderr,Z)

round(table32, 2)
```

__Ex.2__ - Your turn
Reproduce Table 3.3, at least the first four columns that is LS, Best Subset, Ridge and Lasso.
```{r}
Xd = data[,1:8]
n = dim(Xd)[1]
p = dim(Xd)[2]

#X = (Xd - t(matrix(c(rep(colMeans(Xd), n)), ncol =  n)))/ t(matrix(c(rep(c(var(Xd[1]), var(Xd[2]), var(Xd[3]), var(Xd[4]), var(Xd[5]), var(Xd[6]), var(Xd[7]), var(Xd[8])), n)), ncol =  n))
X = scale(Xd)
y = data[9]
```

Training
```{r}
ind = which(data[10] == 1)
Xtrain = X[ind,]
ytrain = y[ind,]
ntrain = max(dim(Xd))
```

Testing
```{r}
ind = which(data[10] == 0)
Xtest = X[ind,]
ytest = y[ind,]
ntest = max(dim(Xd))
```

# LS
```{r}
# CVXR: An R Package for Disciplined Convex Optimization
p <- 9
betaHat <- Variable(p)
Xtrainone <- cbind(array(1, dim = c(nrow(Xtrain),1)), Xtrain)
objective <- Minimize(sum((ytrain  - Xtrainone %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)

# LS
bo <- result$getValue(betaHat)
round(1000*bo)/1000
# Correct values
b <- solve(t(Xtrainone) %*% Xtrainone, t(Xtrainone) %*% ytrain)
round(1000*b)/1000
```


# Best Subset
```{r}
df <- as.data.frame(cbind(Xtrain,ytrain))

best.subset <- regsubsets(ytrain~., df)
best.subset.summary <- summary(best.subset)
best.subset.summary$outmat
```

```{r}
best.subset.by.bic <- which.min(best.subset.summary$bic)
plot(best.subset.summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
points(best.subset.by.bic, best.subset.summary$bic[best.subset.by.bic], col="red", cex =2, pch =20)
```


# Ridge
```{r}
mx = colMeans(Xtrain)
Xc = Xd - t(matrix(c(1, ntrain*p), ncol =  n))
```

# Lasso
```{r}

```
























