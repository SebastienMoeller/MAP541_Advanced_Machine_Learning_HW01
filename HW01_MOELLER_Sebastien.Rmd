---
title: "Homework 1"
author: "Sebastien Moeller"
date: "29/01/2018"
output: html_document
---

## Dependencies
```{r}
if (!require("pacman")) install.packages("pacman")

pacman::p_load(Rfast, CVXR, caTools, leaps, pls)
```

## The objectives of the Lab
The purpose of this lab is to reproduce tables from the third chapter of the book "Elements of
Statistical Learning" from Hastie, Tibshirani and Friedman, as they are shown bellow.
__Ex.1__ Prepare the data
a) Raw data is available on line, download it from moodle (theData.txt file) or from the web at http://statweb.stanford.edu/~tibs/ElemStatLearn.1stEd/datasets/prostate.data.
```{r}
data <- read.table("prostate.data", sep = "")
actualSplit <- data$train
```

b) Extract and normalize the explicative variables
```{r}
X <- scale(data[,1:8])
```

c) Is it wise to normalize these data?

It depends on what we want to do. For a multilinear regression, it might not be needed. But for interpretation purposes it might be clever (eg PCA)

d) Extract the target variable
```{r}
Y <- as.matrix(data[,"lpsa"])
```

e) Split the dataset into training and test data
```{r}
Xtrain <- X[data[["train"]], ]
Ytrain <- Y[data[["train"]],]
Xtest <- X[!data[["train"]], ]
Ytest <- Y[!data[["train"]], ]
```

2. Compute the correlations of predictors in the prostate cancer data as presented Table 3.1
```{r}
Xtrainscale <- scale(Xtrain)

C <- cov(as.matrix(Xtrainscale))
```

3. Reproduce the results presented Table 3.2
a) Compute the coefficients of the linear regression model, without using the lm function (but you can use it validate your code)
```{r}
Xtrainone <- cbind(array(1, dim = c(nrow(Xtrain),1)), Xtrain)

b <- solve(t(Xtrainone) %*% Xtrainone, t(Xtrainone) %*% Ytrain)
```

b) Compute the prediction error
```{r}
Ypred <- Xtrainone %*% b

err <- Ytrain - Ypred
```

c) Compute the standard error for each variable
```{r}
sig2 <- (t(err) %*% err)/ (nrow(Xtrainone) - ncol(X) -1)

v <- diag(solve(t(Xtrainone) %*% Xtrainone))

stderr <- sqrt(as.vector(sig2)) * sqrt(v)
```

d) compute the Z score for each variable
```{r}
Z <- b/stderr
```
e) visualize the results and compare with table 3.2
```{r}
table32 <- cbind(b,stderr,Z)

round(table32, 2)
```




__Ex.2__ â€” Your turn
Reproduce Table 3.3, at least the first four columns that is LS, Best Subset, Ridge and
Lasso.
```{r}
data[32,2] <- 3.8044
X <- scale(data[,1:8])
Y <- as.matrix(data[,"lpsa"])

Xtrain <- X[data[["train"]], ] 
Xtrainscale <- scale(Xtrain)
Ytrain <- Y[data[["train"]],]
Xtest <- X[!data[["train"]], ] 
Ytest <- Y[!data[["train"]], ] 
Xtrainone <- cbind(array(1, dim = c(nrow(Xtrain),1)), Xtrain) 
b <- solve(t(Xtrainone) %*% Xtrainone, t(Xtrainone) %*% Ytrain)
```

```{r}
# check if the package works
p <- 9
betaHat <- Variable(p)
objective <- Minimize(sum((Ytrain  - Xtrainone %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)

bo <- result$getValue(betaHat)
round(1000*bo)/1000
LS <- round(1000*b)/1000
LS
```

```{r}
# error calculation
error <- function(hat){
  # We need to add the intercept, not multiply it
  # Testing error
  Yhattest <- hat[-1] %*% t(Xtest) + hat[1]
  testError <- sum((Ytest - Yhattest)^2)/dim(Xtest)[1]
  # Training error
  Yhattrain <- hat[-1] %*% t(Xtrain) + hat[1]
  trainError <- sum((Ytrain - Yhattrain)^2)/dim(Xtrain)[1]
  
  cbind(testError, trainError)
}
```

```{r}
# Best Subset
p <- 3
betaHat <- Variable(p)
objective <- Minimize(sum((Ytrain  - Xtrainone[,c(1,2,3)] %*% betaHat)^2))
problem <- Problem(objective)
result <- solve(problem)

bo <- result$getValue(betaHat)
BS <- round(1000*bo)/1000
BS
BS <- rbind(BS, NA, NA, NA, NA, NA, NA)
```

```{r}
df <- as.data.frame(cbind(Xtrain,Ytrain))
best.subset <- regsubsets(Ytrain~., df)
best.subset.summary <- summary(best.subset)
best.subset.summary$outmat
```


```{r}
set.seed(1000)
testP <- NA
all_P <- seq(1, 9, 1)

for (j in all_P){
  betaHat <- Variable(j)
  objective <- Minimize(sum((Ytrain  - Xtrainone[,c(1:j)] %*% betaHat)^2))
  problem <- Problem(objective)
  result <- solve(problem)
  betaHat <- result$getValue(betaHat)
}
```


```{r}
# The Ridge
ridge <- function(lambda){
  br <- solve(t(Xtrain) %*% Xtrain + diag(x = lambda, ncol(Xtrain)), t(Xtrain) %*% (Ytrain - mean(Ytrain)))
  br <- rbind(mean(Ytrain), br)
  br
}

Ridge <- round(1000*ridge(24))/1000
Ridge
```




```{r}
all_lambda <- seq(0, 100, 1)

out <- NA
for (i in all_lambda){
  out <- rbind(out, error(ridge(i)))
}
testLambda <- cbind(all_lambda, out[-1,])
```

```{r}
all_lambda <- seq(0, 20, .1)

out <- NA
for (i in all_lambda){
  out <- rbind(out, error(ridge(i)))
}
testLambda <- cbind(all_lambda, out[-1,])

plot(testLambda)

testLambda[which(testLambda[,2] == min(testLambda[,2])),1]
```

```{r}
# 65543
set.seed(1000)
testLambda <- NA
for (j in 1:100){
  data$train <- sample.split(rep(1, dim(data)[1]), SplitRatio = .25)
  Xtrain <- X[data[["train"]], ]
  # scale the data
  Xtrainscale <- scale(Xtrain)
  Ytrain <- Y[data[["train"]],]
  Xtest <- X[!data[["train"]], ] 
  Ytest <- Y[!data[["train"]], ] 
  
  all_lambda <- seq(0, 30, .1)
  
  out <- NA
  for (i in all_lambda){
    out <- rbind(out, error(ridge(i)))
  }
  testLambda <- cbind(testLambda, out[-1,1])
  
  #testLambda[which(testLambda[,2] == min(testLambda[,2])),1]
}
testLambda[,1] <- all_lambda

testLambda2 <- cbind(testLambda[,1], rowmeans(testLambda[,-1]))
plot(testLambda2)

# Returns the value of lambda which had the lowest error
testLambda2[which(testLambda2[,2] == min(testLambda2[,2])),1]
```



```{r}
Ridge <- ridge(24)
Ridge
```


```{r}
# The Lasso
data$train <- actualSplit
Xtrain <- X[data[["train"]], ]
Xtrainscale <- scale(Xtrain)
Ytrain <- Y[data[["train"]],]
Xtest <- X[!data[["train"]], ] 
Ytest <- Y[!data[["train"]], ]

t <- .7015
ys = scale(Ytrain)
betaHat <- Variable(dim(Xtrainscale)[2])
objective <- Minimize(sum((ys - Xtrainscale %*% betaHat)^2))
constraint <- list(sum(abs(betaHat)) <=  t)
problem <- Problem(objective, constraint)
result <- solve(problem)

d <- sqrt(diag(var(Xtrain)))
bl <- result$getValue(betaHat)*sqrt(var(Ytrain)) / d
Lasso <- round(1000*bl)/1000
Lasso <- rbind(round(1000*mean(Ytrain))/1000, Lasso)
Lasso
```



```{r}
#PCR
train <- as.data.frame(cbind(Ytrain, Xtrain))
pcr_model <- pcr(Ytrain ~ ., data = train)

PCR <- as.matrix(pcr_model$coefficients[49:56])
PCR <- rbind(mean(Ytrain), PCR)
PCR
```

```{r}
#PLS
train <- as.data.frame(cbind(Ytrain, Xtrain))
pls_model <- plsr(Ytrain ~ ., data = train)

PLS <- as.matrix(pls_model$coefficients[9:16])
PLS <- rbind(mean(Ytrain), PLS)
PLS
```



Our results correspond to the results of Table 3.3
```{r}
table33 <- as.table(cbind(LS, BS, Ridge, Lasso, PCR, PLS))
rownames(table33)[1] <- 'Intercept'
colnames(table33) <- c('LS', '  Best Subset', '   Ridge', '   Lasso', '     PCR', '     PLS')
round(table33, 3)
```















